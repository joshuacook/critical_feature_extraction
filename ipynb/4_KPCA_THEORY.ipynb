{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kPCA\n",
    "_author: Gus Ostow_\n",
    "\n",
    "-----\n",
    "Before covering kernel methods, I will review the standard and dual algorithms for PCA, as computed by Singular Value Decomposition.\n",
    "\n",
    "### 1. Vanilla PCA\n",
    "\n",
    "Consider an m x n centered data matrix, $X$. An SVD computes the following factorization:\n",
    "\n",
    "$$X = U \\Sigma V^T$$\n",
    "\n",
    "- $U$ is an m x m matrix, where the columns that are eigen-vectors of $XX^T$\n",
    "- $\\Sigma$ is an m x n diagonal matrix, where the diagonal entries are the $r$ non-zero singular values. Note: $r$ is the rank of $X$\n",
    "- $V$ is an n x n matrix, where the columns are the eigen-vectors of $X^TX$ (the covariance matrix). These columns are the principle components of $X$.\n",
    "\n",
    "Here is how to compute common PCA operations in terms of the SVD decomposition:\n",
    "\n",
    "**Transform training set**\n",
    "\n",
    "$$Y = XV_p$$\n",
    "\n",
    "$V_p$ is the n x p truncated $V$ matrix. $p$ is the number of principle components to keep.\n",
    "\n",
    "** Reconstruct training set**\n",
    "\n",
    "$$\\hat{X} = YV_p^T$$\n",
    "$$ = XV_pV_p^T$$\n",
    "\n",
    "Notice that $V_pV_p^T \\neq I$ because they are truncated to $p$ columns, even though $VV^T = I$ by ortho-normality.\n",
    "\n",
    "### 2. Dual PCA\n",
    "\n",
    "Sometimes it is infeasible to directly compute the right singular vectors of $X^TX$ during a standard the standard SVD. Usually this is when there are far more columns than rows ($n >> m$), or in the case of kPCA. Luckily it is possible to solve for $V$ algebraically based on the eigen-vectors of $XX^T$, which might be easier, or only possible to solve for.\n",
    "\n",
    "Start by computing the eigen-decomposition of $XX^T$ to construct $U$ and $\\Sigma$. The diagonal entries of $\\Sigma$ are the roots of the eigen-values of $XX^T$.\n",
    "\n",
    "So now we know $X$, we know $U$ and we know $\\Sigma$. The rest is just matrix algebra to solve for $V$ when you need it.\n",
    "\n",
    "**Transform training set**\n",
    "\n",
    "$$Y = XV_p$$\n",
    "\n",
    "and by SVD, we know that $XV = U\\Sigma$. So\n",
    "$$Y = U_p\\Sigma_p$$\n",
    "\n",
    "** Reconstruct training set**\n",
    "\n",
    "From the standard algorithm for PCA we know that\n",
    "\n",
    "$$\\hat{X} = YV_p^T$$\n",
    "\n",
    "but we don't want to compute $V^T$ directly so we need to solve $XV = U\\Sigma$ for it. We get\n",
    "\n",
    "$$ V_p^T = \\Sigma_p^{-1}U_p^TX$$\n",
    "\n",
    "Plugging everything in, we have the full reconstruction of $X$ from a p-dimensional representation.\n",
    "\n",
    "$$\\hat{X} = U_p\\Sigma_p\\Sigma_p^{-1}U_p^TX$$\n",
    "\n",
    "$$ = U_pU_p^TX$$\n",
    "\n",
    "\n",
    "### 3. Kernel PCA\n",
    "\n",
    "An assumption of standard PCA is that the structure of the data is best represented by a linear subspace. When linearity is ill-assumed, kPCA captures dynamics that lay on a sub-_**manifold**_, a potentially curved surface. The strategy: unfold the manifold into a higher dimensional space, where PCA might work better.\n",
    "\n",
    "_**Definition:**_ \n",
    "$\\Phi(x)$ is a function that transforms an $n$ dimensional vector into a $d$ dimensional vector, where $d >> n$, if not infinite-dimensional. \n",
    "\n",
    "_**Definition:**_ \n",
    "$K(x, y) = \\langle\\Phi(x), \\Phi(y)\\rangle$\n",
    "\n",
    "_**Example:**_ \n",
    "Consider $X$ a 2 x 1 vector, then let $$\\Phi(X) = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ \\sqrt{2} x_1x_2  \\end{bmatrix}$$\n",
    "\n",
    "If you wanted to calculate the inner product of X and Y in this 3-dimensional space $\\langle\\Phi(X), \\Phi(Y)\\rangle$, you could figure out the dot product by hand, or you could use the kernel\n",
    "\n",
    "$$K(X, Y) = (\\langle X, Y\\rangle)^2$$\n",
    "\n",
    "Test it out yourself. They are equivalent. Anytime you want to compute a dot product in a higher-dimensional space you can use the kernel instead of actually expanding vectors into that space. THAT is the kernel trick... that's the swindle. You get to bypass the high dimensional spaces all together. They are only *implicit*.\n",
    "\n",
    "If the dot product is a measure of vector similarity, you can think of kernels as a more-expressive, non-linear similarity measures than what is typically just an unscaled cosine similarity. We use these new similarity measures to construct a richer covariance matrix via the dual PCA algorithm.\n",
    "\n",
    "Anywhere you see the dot product of data samples, you replace it with action by the kernel function. So now on to the algorithm.\n",
    "\n",
    "Just as dual PCA starts with the eigen-decomposition of $XX^T$, kPCA decomposes $\\Phi(X)\\Phi(X^T)$, which we do not have to explicitely execute. Instead, leveraging the kernel trick, generate the columns of U and the diagonal elements of $\\Sigma$ as the eigen-vectors of $K(X, X^T)$.\n",
    "\n",
    "**Transform training set**\n",
    "\n",
    "We transform the training set exactly the same as in dual PCA, using the \"kernalized\" $U$ matrix.\n",
    "\n",
    "$$Y = U_p\\Sigma_p$$\n",
    "\n",
    "** Reconstruct training set**\n",
    "\n",
    "You cannot reconstruct data using the kernel trick. And here's why. From dual PCA we have\n",
    "\n",
    "$$\\hat{X} = U_pU_p^TX$$\n",
    "\n",
    "We might not necessarily be able to compute $\\Phi(X)$ on it's own needed in the above. We do not have the inner product to convert using the kernel. Depending on the kernel, some reconstructions might not even exist.\n",
    "\n",
    "### Sources\n",
    "\n",
    "Ali Ghodsi, University of Waterloo - https://www.youtube.com/watch?v=jeOEXCFK30M&t=836s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
